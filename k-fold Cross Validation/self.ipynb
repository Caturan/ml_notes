{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to k-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-validation is a statistical method used to estimate the skill of machine learning models. \n",
    "- It has a lower bias than other methods. \n",
    "- You want to evaluate the model multiple times so you can be more confident about the model design. \n",
    "- The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. \n",
    "- To use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model. \n",
    "- Note that k-fold cross-validation is to evaluate the model design, not a particular training. Because you re-trained the model of the same design with different training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general procedure: \n",
    "1. Shuffle the dataset randomly. \n",
    "2. Split the dataset into k groups. \n",
    "3. For each unique group:\n",
    "    1. Take the group as a hold out test data set\n",
    "    2. Take the remaining groups as a training data set \n",
    "    3. Fit a model on the training set and evaluate it on the test set \n",
    "    4. Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importantly, each observation in the data sample is assigned to an individual group and stays in taht group for the duration of the procedure. \n",
    "- This means that each sample is fiven the opportunity to be used in the hold out set 1 time and used to train the model k-1 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of k \n",
    "- The choice of k is usually 5 or 10, but there is no formal rule. \n",
    "- As k gets larger, the difference in size between the training set and resampling subsets  gets smaller. \n",
    "- As this difference decreases, the bias of the technique becomes smaller. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
