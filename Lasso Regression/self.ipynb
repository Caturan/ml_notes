{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO -short introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LASSO (Least Absolute Shrinkage and Selection Operator), similar to ridge regression, is a certain modification of linear regression. \n",
    "- Ridge regression is specifically designed to address multcollinearity in the dataset. The LASSO method has a completely different but also useful advantage. \n",
    "- It performs both feature selection and regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "- Feature selection in machine learning involves selecting a subset of relevant features (or variables) from the original set of features available in the dataset. \n",
    "- The goal of feature selection is to improve model performance by reducing the dimensionality of the feature space, removing irrelevant or reduntant features, and focusing only on those features that contribute most to the predictive power of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization \n",
    "- Refers to the technique of adding a penalty term to the objective function during training to prevent overfitting and improve generalization performance. \n",
    "- The primary goal of regularization is to discourage the model from fitting the training data too closely, which can lead to poor performance on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge regression also introduces a regularization term that penalizes large coefficients. \n",
    "- In the case of ridge regression, the regularization term is known as the L2 penalty. \n",
    "- In LASSO, the regularization term has a slightly different form and is called the L1 penalty. \n",
    "- Both penalties forms serve a similar purpose in terms of regularization, but their different forms mean that each of them regularizes the model in a different way. \n",
    "- Additionalyy, each penalty form has a different properties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso --definition\n",
    "- In LASSO, we also minimize the RSS, however, augmented by a regularization term called the L1 penalty.\n",
    "- ![alt text](image.png)\n",
    "- The parameter lambda is a tuning parameter that determines how much penalty we impose on the model for having excessively large coeeficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between LASSO and ridge regression? \n",
    "- The expression we minimize in these models only differs in norm we use for the penalty. \n",
    "- In ridge regression, we used the norm p=2, while in LASSO we use the norm p=1. \n",
    "- ![alt text](image-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "- The key to an effective solution for LASSO became the coordinate descent algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the coordinate descent? \n",
    "1. Start with an initial guess for the parameters. \n",
    "2. For each paramater k=1,..p:\n",
    "    2.1 Iteratevly solve the single variable optimization problem. \n",
    "    2.2 Update the parameter in a way that minimizes the objective function with respect to that parameter alone and repeat this process for each parameter until convergence criteria are met. \n",
    "\n",
    "- The key idea behind coordinate descent is that, for many optimization problems, updating one parameter at a time can be simpler and more computationally efficient than updating all paramters simultaneously. \n",
    "- This approach is particulary well-suited for problems with a large number of parameters or when the objective funciton is not differentiable o rnot easily optimized using gradient-based methods.\n",
    "- ![alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LASSO forces some coeeficients to be exactly zero, it is useful for identifying unimportant features that can be dropped from the model. \n",
    "- Regularization is a technique used in machine learning to penalize complex models to protect them from overfitting. \n",
    "- By doing this, regularization helps to prevent models from over-interpreting the noise and randomness found in data sets. \n",
    "- Lasso regularization encourages sparsity by forcing some coefficients to reduce their values until they eventually become zero while others remain unaffrected or shrink less dramatically. \n",
    "- Ridge regularization can be more effective than Lasso when there are many collinear variables because it prevents individual coefficients from becoming too large and overwhelming others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantes and Disadvantages\n",
    "- One of the main advantages of Lasso Regression is its ability to perform feature selection. \n",
    "- Can help to reduce the complexity of the model and improve its interpretability(yorumlanabilmek). \n",
    "- Lasso is computationally efficient and can handle a large number of features, which makes it suitable for high-dimensional datasets. \n",
    "\n",
    "- However, Lasso Regression also has some limitations. \n",
    "- One of the main disadvantages is that it is not well-suited for datasets with correlated features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
