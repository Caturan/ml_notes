{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient, in plain terms means slope or slant of a surface. \n",
    "- So gradient descent literally means descending a slope to reach the lowest point on that surface. \n",
    "- Let us image a two dimensional graph, such as a parabola in the figure below: \n",
    "- ![alt text](image.png)\n",
    "- In the above graph, the lowest point on the parabola occurs at x =1. \n",
    "- The objective of gradient descent algorithm is to find the value of \"x\" such that \"y\" is minimum. \n",
    "- \"y\" here is termed as the objective function that the gradient descent algorithm operates upon, to descent to the lowest point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "- The objective of regression, is to minimize the sum of squared residuals. \n",
    "- Function reaches its minimum value when the slope is equal to 0.\n",
    "- By using this technique, we solved the linear regression problem and learnt the weight vector.\n",
    "- Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to move down in steps? \n",
    "- This is the crux(en önemli nokta) of the algorithm. \n",
    "- The general idea is to start with a random point and find a way to update this point with each iteration such that we descent(iniş) the slope. \n",
    "- The steps of the algorithm are:\n",
    "1. Find the slope of the objective function with respect to each paramter/feature. In other words, compute the gradient of the function. \n",
    "2. Pick a random initial value for the parameters. (To clarify, in the parabola example, differentiate \"y\" with respect to \"x\". If we had more features like x1,x2 etc.., we take the partial derivative of \"y\" with respect to each of the features.)\n",
    "3. Update the gradient function by plugging in the parameter values. \n",
    "4. Calculate the step sizes for each feature as: step size = gradient * learning rate. \n",
    "5. Calculate the new parameters as: new params = old params - step size \n",
    "6. Repeat steps 3 to 5 until gradient is almost 0. \n",
    "\n",
    "- The \"learning rate\" mentioned above is a flexible parameter which heavily influences the convergence of the algorithm.  \n",
    "- Larger learning rates make the algorithm take huge steps down the slope and it might jump across the minimum point thereby missing it. \n",
    "- So, it is always good to stick to low learning rate such as 0.01.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SDG)\n",
    "- With large data it will be so much computations will have. \n",
    "- Gradient descent is slow on huge data.\n",
    "- Stochastic gradient descent comes to our rescu !! \"Stochastic\", in plain terms means \"random\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where can we potentially induce randomness in our gradient descent algorithm? \n",
    "- While selecting data points at each step to calculate the derivatives. \n",
    "- SGD randomly picks one data point from the whole data set at each iteration to reduce the computataions enormously.\n",
    "- It is also common to sample a small number of data points instead of just one point at each step and that is called \"mini-batch\" gradient descent. \n",
    "- Mini-batch tries to strike a balance between the goodness of gradient descent and speed of SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent vs. Gradient Descent \n",
    "- Speed: SGD is much faster than GD, especially when the dataset is large.\n",
    "    - This is because SGD only updates the model parameters after each individual data point,\n",
    "    - while GD updates the parameters after each batch of data points. \n",
    "- Robustness to noise: SGD is more robust to noise in the data than GD.\n",
    "    - This is because SGD only uses a single data point to update the parameters,\n",
    "    - so it is less likely to be affected by outliers or noise in the data.\n",
    "- Efficiency: SGD is more efficient than GD in terms of memory usage.   \n",
    "    - This is because SGD only needs to store the current parameters and the gradient of the loss function, while GD needs to store the entire dataset. \n",
    "\n",
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical facts\n",
    "- A smaller learning rate will converge more slowly, but it will be more stable. \n",
    "- The choice of batch siz can also affect the speed of convergence. \n",
    "    - A smaller batch size will converge more slowly, but it will be more accurate.\n",
    "    - A larger batch size will converge more quickly, but it may less accurate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
