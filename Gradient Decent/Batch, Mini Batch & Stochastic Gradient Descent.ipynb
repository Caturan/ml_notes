{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "- Gradinet descent is a first-order iterative optimization algorithm for finding the mininmum of a function. \n",
    "- There are different ways in which that man (weights) can go down the slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "- In Batch Gradient Descent, all the training data is taken into consideration to take a single step.\n",
    "- We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. \n",
    "    - So that's just one step of gradient descent in one epoch. \n",
    "- Batch Gradient Descent is great for convew or relatively smooth error manifolds. \n",
    "    - In this case, we move somewhat directly towards an optimum solution. \n",
    "- ![alt text](image-2.png)\n",
    "- The graph of cost vs epochs is also quite smooth because we are averaging over all the gradient of trainin data for a single step.\n",
    "- The cost keeps on decreasing over the epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent \n",
    "- In Batch Gradient Descent we were considering all the examples for every step of Gradient Descent. But what if our dataset is very huge. Deep learning models crave for data. \n",
    "- To tackle this problem we have Stochastic Gradient Descent. \n",
    "- In Stochastic Gradient Descent, we consider just one example at a time to take a single step. \n",
    "- We do the following in one epoch for SGD:\n",
    "1. Take an example \n",
    "2. Feed it to Neural Network \n",
    "3. Calculate it's gradient\n",
    "4. Use the gradient we calculated in step 3 to update the weights \n",
    "5. Repeat steps 1-4 for all the examples in training dataset. \n",
    "\n",
    "- Since we are considering just one example at a time the cost will fluctuate over the training examples and it will bot necessarily decrease. \n",
    "    - But in the long run, we will see the cost decreasing with fluctations.\n",
    "- ![alt text](image-3.png)\n",
    "- Also  because the cost is so fluctuating, it will never reach the minima but it will keep dancing around it. \n",
    "- SGD can be used for larger dataset. It converges faster when the dataset is large as it causes updates to the parameters more frequently. \n",
    "- ![alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Gradient Descent \n",
    "- Batch Gradient Descent can be used for smoother curves. \n",
    "- SGD can be used when the dataset is large. \n",
    "- Batch Gradient Descent converges directly to minima. \n",
    "- SDG converges faster for larger datasets. \n",
    "- But, since in SGD we use only one example at a time, we cannot implement the vectorized implementation on it. This can slow down the computations. \n",
    "- To tackle this problem, a mixture of Batch Gradient Descent and SGD is used. \n",
    "- We use a batch of fixed number of training examples which is less than the actual dataset and call it a mini-batch. \n",
    "\n",
    "- So, after creating the mini-batches of fixed-size, we do the following steps in one epoch:\n",
    "1. Pick a mini-batch \n",
    "2. Feed it to Neural Network \n",
    "3. Calculate the mean gradient of the mini-batch \n",
    "4. Use the mean gradient we calculated in step 3 to update the wieghts \n",
    "5. Repeat stps 1-4 for the mini-batches we created\n",
    "\n",
    "- Just like SGD, the average cost over the epochs in mini-batch gradient descent fluctuates because we are averaging a small number of examples at a time.\n",
    "- ![alt text](image-5.png)\n",
    "- So, when we are using the mini-batch gradient descent we are updating our parameters frequently as well as we can use vectorized implementation for faster computations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
