{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will talk about one such widely used machine learning classification tehnique called the k-nearest neighbors (KNN) algorithm.\n",
    "- The k-neares neighbors (KNN) algorithm is a simple, supervised machine learning method that makes predictions based on how close a data point is to othres. \n",
    "- It's widely used fot both classification and regression tasks because of its simplicity and popularity. \n",
    "- The algorithm identifies the K nearest neighbors to the input data point based on their distances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Do We use the KNN Algorithm? \n",
    "- KNN Algorithm can be used for both classification and regression predictive problems. \n",
    "- However, it is more widely used in classification problems in the industry. \n",
    "- To evaluate any technique, we generally look at 3 important aspects: \n",
    "1. Ease of interpreting output\n",
    "2. Calculation time \n",
    "3. Predictive Power "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Choose the Factor K? \n",
    "- The boundary becomes smoother with increasing value of K. With K increasing to it finally becomes all blue or all red depending on the total majority. \n",
    "- The training error rate and the validation error rate are two parameters we need to access different K-value. \n",
    "- Following is the curve for the training error rate with a varying value of K: \n",
    "- ![alt text](image.png)\n",
    "- As you can see, the error rate at K=1 is always zero for the training sample. This is because the closest point to any training data point is itself. Hence the prediction is always accurate with K=1. \n",
    "\n",
    "- If validation error curve would have been similar, our choice of K would have been 1. \n",
    "- Following is the validation error curve with varying value of K:\n",
    "- ![alt text](image-1.png)\n",
    "- This makes the story more clear. At K=1, we were overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. \n",
    "- After the minima point, it then increase wtih increasing K. \n",
    "- To get the optimal value of K, we can segregate the training and validation from the initial dataset. \n",
    "- This value of K should be used for all predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- KNN classifier operates by finding the K nearest neighbors to a given data point, and it takes the majority vote to classify the data point. \n",
    "- The value of k is crucial, and one needs to choose it wisely to prevent overfitting or underfitting the model. \n",
    "- One can use cross-validation to select the optimal value of k for the k-NN algorithm,  \n",
    "    - which helps improve its performance and prevent overfitting or underfitting. \n",
    "    - Cross validation is also used to identify the outliers before applying the KNN algorithm."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
