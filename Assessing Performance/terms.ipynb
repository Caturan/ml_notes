{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function is a critical concept in Machine Learning that measures how far off the model's predicitons (^y) are from the actual (true) values (y). It quantifies the \"error\" or \"loss\" of a model, and the goal during training is to minimize this loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need a Loss Function? \n",
    "The loss function acts as a bridge between the model's predctions and reality:\n",
    "1. It guides the learning process. \n",
    "2. It updates model parameters during optimization (using gradient descent). \n",
    "3. It helps assess how good or bad a model is performing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regression Loss Functions: Mean Squared Error(MSE), Mean Absolute Error (MAE) is type of Loss Functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Absolute Error (L1 Loss) and Squared Error (L2 Loss) are two common loss function used to measure the error (or \"loss\"). \n",
    "### Absolute Error (L1 Loss):\n",
    "- Treats all errors equally because the error grows linearly. \n",
    "- It is robust outliers since large errors are not amplified. \n",
    "- Non-differentiable at y = ^y, which can make optimization harder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squared Error (L2 Loss):\n",
    "- Errors grow quadratically, meaning larger errors are amplified. \n",
    "- Very sensitive to outliers because squareing large errors makes them much bigger. \n",
    "- Smooth and differentiable, which makes it easier to optimize with techniques like gradient descent. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
