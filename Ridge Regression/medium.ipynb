{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Introduction \n",
    "- Ridge regression is a varioation of linear regression, specifically designed to address multicollinerarity in the dataset.\n",
    "- In linear regression, the goal is to find the best-fitting hyperplane that minimizes the sum of squared differences between the observed and predicted values. However, when there are highly correlated variables, linear regression may become unstable and provide unreliable estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multicollinearity exist when two or more of the predictors in a regression model are moderately or highly correlated with one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge regression introduces a regularization term that penalizes large coefficients, helping to stabilize the model and prevent overfitting.\n",
    "- The regularization term, also known as the L2 penalty, adds a constraint to the optimization process, influencing the model to choose smaller coefficinets for the predictors. \n",
    "- By striking a balance between fitting the data well and keeping the coefficients in check, ridge regression proves valuable in improving the robustness and performance of linear regression models, espcially in situations with multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression --for the good star\n",
    "- Let's briefly recall what linear regression was about. \n",
    "- In linear regression, the model training essentially involves finding the appropriate values for coefficients. \n",
    "- This is done using the method of least squares. One seeks the values B0,B1,... that minimize the Residual Sum of Squares: \n",
    "- ![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression --definition \n",
    "- Ridge regression is very similar to the method of least squares, with the exception that the coefficients are estimated by minimizing a slightly different quantity. \n",
    "- In reality, it's the same quantity, just with soething more, with something we call a shrinkage penalty. \n",
    "- ![alt text](image-1.png)\n",
    "- Before we explain what ridge regression is, let'S find out what the mysterious shrinkage penalty is all about. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage penalty --aid in learning \n",
    "- The shrinkage penalty in ridge regression \n",
    "- ![alt text](image-2.png)\n",
    "- refers to the regularization term added to the linear regression equation to prevent overfitting and address multicollinearity. \n",
    "- In ridge regression, the objective is to minimized the sum of squared differences between observed and predicted values. \n",
    "- However, to this, a penalty term is added, which is proportional to the square of the magnitude of the coefficients. \n",
    "- This penalty term is also known as the l2 norm Euclidean norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ðœ†â‰¥0 is called the tuning parameter of the method, which is chosen separately. \n",
    "- The parameter ðœ† controls how strongly the coefficients are shrunk toward 0. \n",
    "- When ðœ†=0, the penalty has no effect, and ridge regression reduces to the ordinary least squares method. \n",
    "- However, as ðœ†â†’âˆž the impact of the penalty grows, and the estimates of the coefficients Bj in ridge regression shrink towards zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to chosee ðœ†?\n",
    "- At the beginning, it's not known. \n",
    "- The only way is to test many values, and that's typically how it's done. \n",
    "- However, there are many algorithm implementations that asist in selecting the appropriate ðœ† like \"cross-validation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why you should sclade predictors? \n",
    "- It should also be notes that the shrinkage penalty is applied exclusively to the coefficients B1,...Bp, but it does not affect the intercept term B0. \n",
    "- We do not shrink the intercept -- it represents the prediction of the mean value of the dependent variable when all predictors are equal to 0. \n",
    "- Assuming that the variables have been centered to have a mean of zero before conductiong ridge regression, the estimated intercept will take form \n",
    "- ![alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It should be emphasized that scaling predictors matters. \n",
    "- In linear regression, multiplying the predictor Xj by a constand c reduces the estimated parameter by 1/c (meaning XjBj remains unchanged). \n",
    "- However, in ridge regression, due to shrinkage penalty, scaling the predictor Xj can significantly change both the estimated parameter Bj and other predictions. \n",
    "- Therefore, before applying ridge regression, predictors are standardized to be on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature standardization is a preprocessing step in machine learning where the input features are tranformed to have a mean of 0 and a standard deviation of 1. \n",
    "- This is typically achieved by subtracting the mean of each feature from its values and then dividing by the standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance tradeoff of the ridge estimator\n",
    "- The superiority(Ã¼stÃ¼nlÃ¼k) of ridge regression compared to the method of least squares arises from the inherent trade-off between variance and bias. \n",
    "- Ridge regression introduces a regularization parameter, denoted as ðœ†, which control the extent of shrinkage applied to the regression coefficients. \n",
    "- As the value of ðœ† increases, the model's flexibility in fitting the data diminishes (kÃ¼Ã§Ã¼ltmek). \n",
    "- Consequenlty, this decrease in flexibility results in a simultaneous reduction in variance but an increase in bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's notice:\n",
    "- When the number of predictors, p, is close to the number of observations, n, the method of least squares exhibits high variance -a small change in the traning data can lead to a significant change in the estimated parameters. \n",
    "- When p>n, the method of least squares stops working (due to the lack of estimation uniqueness), whereas ridge regression handles this situation well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Ridge Regression Balance and Variance \n",
    "- 1. High Variance (Overfitting): \n",
    "    - When a model has too many parameters or the regularization is too weak (Î» is small), it can fit the noise in the training data. \n",
    "    - Ridge regression reduces variance by shrinking the model coefficients toward zero, making the model simpler and less sensitive to noise. \n",
    "- 2. High Bias (Underfitting):\n",
    "    - When Î» is too large, the penalty term dominates, and the model coefficients are overly shrunk. \n",
    "    - This results in high bias as the model fails to capture the true underlying patterns in the data. \n",
    "3. Tradedoff Mechanism: \n",
    "    - Î» controls the tradeoff between bias and variance:\n",
    "        - Small Î»: Lower bias, higher variance. \n",
    "        - Large Î»: Higher bias, lower variance. \n",
    "    - The optimal Î» balances the two to minimize the total error (sum of bias squared, variance and irreducible error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Application\n",
    "- To find the optimal Î», cross-validation is typically used. It splits the dataset into traning and validation sets to evaluate the model's performance for different values of Î», ensuring a balacnced tradeoff between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datasciencedecoded.com/posts/8_Ridge_Regression_for_Improved_Predictive_Models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
