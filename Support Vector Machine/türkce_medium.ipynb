{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nedir Bu Destek Vektör Makineleri? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Destek Vektör Makineleri genellikle sınıflandırma problemlerinde kullanılan gözetimli öğrenme yöntemlerinden biridir. \n",
    "- Bir düzlem üzerine yerleştirilimiş noktaları ayırmak için bir doğru çizer. \n",
    "- Bu doğrunun, iki sınıfının noktaları için de maksimum uzaklıkta olmasını amaçlar. \n",
    "- Karmaşık ama küçük ve orta ölçekteki veri setleri için uygundur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ![alt text](image.png) \n",
    "- Tabloda siyahlar ve beyazlar olmak üzere iki farklı sınıf var. \n",
    "- Sınınflandırma problemlerindeki asıl amacımız gelecek verinin hangi sınıftaki yer alacağını karar vermektir. \n",
    "- Bu sınıflandırmayı yapabilmek için iki sınıfı ayıran bir doğru çizilir ve bu doğrunun +-1'i arasında kalan yeşil bölgeye Margin adı verilir. \n",
    "- Margin ne kadar geniş ise iki veya daha fazla sınıf o kadar iyi ayrıştırlır. \n",
    "- ![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Margin vs Soft Margin \n",
    "- Marginimiz her zaman bu şekilde olmayabilir. Bazen örneklerimiz Margin bölgesine girebilir. \n",
    "    - Buna Soft Margin denir. \n",
    "- Hard Margin, verimiz doğrusal olarak ayrılabiliyorsa çalışır ve aykırı değerlere karşı çok duyarlıdır. \n",
    "- Bu yüzden bazı durumlarda Soft Margini tercih etmemiz gerekebilir. \n",
    "- ![alt text](image-2.png)\n",
    "- İkili arasındaki dengeyi SVM içerisindeki C hiperparametresi ile kontrol edebiliriz. \n",
    "    - C ne kadar büyükse Margin o kadar dardır. \n",
    "- ![alt text](image-3.png)\n",
    "- Ayrıca model overfit olursa C'yi azaltmamız gerekir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick \n",
    "- Düşük boyulu karmaşık veri setlerini açıklamada yeterli olmayabilir.  \n",
    "- Boyutu arttırsak işlemler artacağı için çok uzun sürer.\n",
    "- İşte Kernel Trick burada devreye giriyor. \n",
    "- Elimizdeki koordinatları belirli Kernel Fonksiyonları ile çarparak çok daha anlamlı hale getirebiliyoruz. \n",
    "- Bu yöntemlerden en çok kullanılan ikisi: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) Polynomial Kernel:\n",
    "- Bu yöntemde problemimizi çözmek için 2 boyuttan çıkıp 3 veya daha fazlası boyutta işlem yapıyormuş gibi hareket ediyoruz. \n",
    "- ![alt text](image-4.png)\n",
    "- Soldaki dağılımı bir doğru ile sınıflandıramayız. Bunun için bu gibi problemlerde Polynomial Kernel'i kullanabiliriz. \n",
    "- 3. boyutta işlem yaparken sınıflara ayırmak için doğru yerine bir düzlem kullanırız ve çok daha düzgün bir şekilde sınıflandırabiliriz. \n",
    "- Modelimiz overfit olmuşsa derecesini düşürmeniz, underfit olmuşsa derecesini yükseltmemiz gerekir. \n",
    "- Ayıca coef0 hiperparametresiyle modelimizin yüksek dereceli denklemlerden ne kadar etkileneceğini ayarlayabilirsiniz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) Gaussian RBF (Radial Basis Function) Kernel: \n",
    "- Anlaması biraz güç olabilir ama sonsuz boyuttaki Destek Vektör Makinelerini bulur ve her bir noktanın belirli bir noktaya ne kadar benzediğini normal dağılım ile hesaplar, one göre sınıflandırır. \n",
    "- Dağılımın genişliğini gamma hiperparametresi ile kontrol ederiz. \n",
    "- Gamma ne kadar küçükse dağılım o kadar geniş olur. \n",
    "- C hiperparametresindeki gibi, model overfit olmuşsa gamma değerini düşürmemiz, model overfit olmuşsa gamma değerini yükseltmemiz gerekir. \n",
    "- ![alt text](image-5.png)\n",
    "- Veri setiniz aşırı büyük değilse genellikle RBF kernel tercih edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Özet \n",
    "1. Destek Vektör Makineleri (SVM), düzlem üzerindeki noktaların bir doğru veya hiper düzlem ile ayrıştırılması ve sınıflandırılmasıdır.\n",
    "\n",
    "2. Küçük veya orta büyüklükteki veri setleri için uygundur. Scale’e duyarlıdır. Scale edilmesi gerekir.\n",
    "\n",
    "3. Hard Margin ve Soft Margin arasındaki dengeyi C ile kontrol edebiliriz. C büyüdükçe Margin daralır.\n",
    "\n",
    "4. Model overfit olursa C’nin azlatılması gerekir.\n",
    "\n",
    "5. 2 boyutta açıklanamayan değişimleri boyut arttırarak çözüyormuş gibi yapılan hilelere Kernel Trick denir.\n",
    "\n",
    "6. 2 boyutta açıklayamadığımız veri setimizi daha fazla boyutta açıklamak için kullanılan Kernel Trick metoduna Polynomial Kernel denir.\n",
    "\n",
    "7.  Model overfit olursa derecesi düşürülür, underfit olursa derece yükseltilir. Coef0 hiperparametresi ile yüksek dereceli denklemlerden ne kadar etkileneceğini ayarlayabilirsiniz.\n",
    "\n",
    "8. Her bir noktanın belirli bir noktaya ne kadar benzediğini normal dağılım ile hesaplayan, ona göre sınıflandıran Kernel Trick metoduna RBF Kernel denir.\n",
    "\n",
    "9. Dağılım genişliğini kontrol ettiğimiz gamma değeri ne kadar küçükse dağılım o kadar geniş olur. Model overfit olmuşsa gamma değerini düşürmemiz, model underfit olmuşsa gamma değerini yükseltmemiz gerekir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/deep-learning-turkiye/nedir-bu-destek-vekt%C3%B6r-makineleri-makine-%C3%B6%C4%9Frenmesi-serisi-2-94e576e4223e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
